# RAG PDF Q&A (Local LLM via Ollama)

A Streamlit + LangChain app that ingests PDF resumes, builds a local Chroma vector store with SentenceTransformers embeddings, and answers questions using a **local Ollama model** (default `llama3`). Everything runs locally; ensure the Ollama daemon is running.

## ğŸš€ Features

- PDF upload (multi-file) and ingestion
- Chunking with configurable size/overlap
- Embeddings with `sentence-transformers/all-MiniLM-L6-v2`
- Vector store persisted with Chroma
- Local LLM via Ollama (default: `llama3`)
- Source snippets displayed with page numbers
- Docker and GitHub Actions support

## ğŸ“‹ Prerequisites

- Python 3.10+ (Dockerfile uses 3.10)
- Ollama installed and running locally (`ollama --version`)
- Docker (optional)  
- GPU optional; pull a GPU-friendly model in Ollama if available

## ğŸ› ï¸ Installation

### Quick Start (local)

1) Create venv & install deps  
```bash
python -m venv venv
source venv/bin/activate            # Windows: venv\Scripts\activate
pip install --upgrade pip
pip install -r requirements.txt
```

2) Ensure an Ollama model is pulled (default `llama3`):
```bash
ollama pull llama3
```

3) Run Streamlit  
```bash
streamlit run app.py
```
Open http://localhost:8501.

> Quick helpers: `run.bat` (Windows) or `run.sh` (Unix). Make sure Ollama is running.

## ğŸ³ Docker

Build:  
```bash
docker build -t utils-resume-chatbot .
```
Run:  
```bash
docker run -p 8501:8501 utils-resume-chatbot
```
Persist Chroma DB:  
```bash
docker run -p 8501:8501 -v $(pwd)/data/vectordb:/app/data/vectordb utils-resume-chatbot
```
GPU models (if you use Ollama GPU models):  
```bash
docker run --gpus all -p 8501:8501 utils-resume-chatbot
```

## ğŸ“– Usage

1) Upload one or more PDFs (sidebar)  
2) Click **â€œIngest PDFs & Build Vector DBâ€**  
3) Ask a question in the main panel and click **â€œGet Answerâ€**  
4) View retrieved sources with page numbers

### Example Questions

- "What are the key skills mentioned in this resume?"
- "What is the candidate's work experience?"
- "What educational background does the candidate have?"
- "What are the main responsibilities mentioned?"
- "What technologies or tools are mentioned?"

## ğŸ—ï¸ Architecture (code refs)

- `loaders/pdf_loader.py` â€” loads PDFs with `PyPDFLoader`, adds filename metadata
- `utils/text_splitter.py` â€” `RecursiveCharacterTextSplitter`
- `embeddings/embedding.py` â€” cached `HuggingFaceEmbeddings` (MiniLM)
- `vectorstore/store.py` â€” build/load Chroma persistent store
- `llm/local_llm.py` â€” Ollama client using `config.py`
- `rag/chain.py` â€” simple RAG pipeline (retriever â†’ prompt â†’ LLM)
- `app.py` â€” Streamlit UI wiring everything together

## ğŸ“ Project Structure

```
.
â”œâ”€â”€ app.py                  # Streamlit UI
â”œâ”€â”€ config.py               # Paths + hyperparams + model path
â”œâ”€â”€ loaders/pdf_loader.py   # PDF ingestion
â”œâ”€â”€ utils/text_splitter.py  # Chunking
â”œâ”€â”€ embeddings/embedding.py # Embedding model
â”œâ”€â”€ vectorstore/store.py    # Chroma persistence
â”œâ”€â”€ llm/local_llm.py        # LlamaCpp wrapper
â”œâ”€â”€ rag/chain.py            # RAG chain
â”œâ”€â”€ data/                   # uploads + vectordb
â”œâ”€â”€ storage/faiss_index/    # legacy FAISS artifacts (if any)
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ docker-compose.yml
â””â”€â”€ README.md
```

## ğŸ”§ Configuration

Key knobs in `config.py` / UI:
- `CHUNK_SIZE` / `CHUNK_OVERLAP`
- `TOP_K` (retrieval)
- `OLLAMA_MODEL`, `OLLAMA_BASE_URL`, `TOP_K`, chunk sizes

## ğŸ› Troubleshooting

### Ollama not reachable
- Make sure the Ollama service is running: `ollama list` should work.
- Update `OLLAMA_BASE_URL` in `config.py` if running remotely or in Docker.

### Retrieval quality
- Tune chunk size/overlap and `TOP_K` in the sidebar.

## ğŸ“ License

This project is open source and available under the MIT License.

## ğŸ¤ Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

## ğŸ“¦ GitHub Setup

### Initializing Git Repository

1. **Initialize git repository**
   ```bash
   git init
   ```

2. **Add all files**
   ```bash
   git add .
   ```

3. **Create initial commit**
   ```bash
   git commit -m "Initial commit: RAG Resume Chatbot with Streamlit"
   ```

4. **Add remote repository** (replace with your GitHub repo URL)
   ```bash
   git remote add origin https://github.com/yourusername/rag-resume-chatbot.git
   ```

5. **Push to GitHub**
   ```bash
   git branch -M main
   git push -u origin main
   ```

### GitHub Repository Structure

Make sure your GitHub repository includes:
- âœ… `app.py` - Main application
- âœ… `requirements.txt` - Dependencies
- âœ… `Dockerfile` - Docker configuration
- âœ… `docker-compose.yml` - Docker Compose setup
- âœ… `README.md` - Documentation
- âœ… `.gitignore` - Git ignore rules
- âœ… `.dockerignore` - Docker ignore rules
- âœ… `LICENSE` - License file

## ğŸ“§ Contact
[Md Ruhul Amin](https://www.linkedin.com/in/ruhul-duet-cse/); \
Email: ruhul.cse.duet@gmail.com

For questions or issues, please open an issue on GitHub.

---

The first run downloads embedding weights; keep an internet connection for that step.

